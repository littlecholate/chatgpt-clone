**Problem**  
Running vLLM in Docker with Qwen/Qwen3-0.6B failed with:

ValueError: The model's max seq len (40960) is larger than the maximum number of tokens that can be stored in KV cache (9888). Try increasing gpu_memory_utilization or decreasing max_model_len.

**Cause**

-   My GPU (GTX 1650 Super, 4GB VRAM) cannot allocate enough KV cache for the full 40k context length of the model.
-   Default vLLM startup tries to honor the full model context window â†’ mismatch with available memory.

**Solution**

-   Limit the context length and adjust GPU memory utilization when starting the container:

```bash
docker run --runtime nvidia --gpus all \
  --name my_vllm \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  --env "HUGGING_FACE_HUB_TOKEN=<YOUR_TOKEN>" \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen3-0.6B \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.92
```
